{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T10:09:55.192497Z",
     "start_time": "2018-11-01T10:09:55.189708Z"
    },
    "ipub": {
     "ignore": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=10>Classification and Regression from linear and logistic regression to neural networks</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/anacost/project2-FYS-STK4155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General structure\n",
    "\n",
    "- Project title\n",
    "- Name, email, course title, date, group assistant\n",
    "- Abstract (1/2 page max)\n",
    "- Introduction (1 page)\n",
    "- Method\n",
    "    - Packages used\n",
    "    - Datasets (models and observations)\n",
    "    - Analysis method\n",
    "    - â€¦\n",
    "- Results\n",
    "- Discussion and outlook (1 page)\n",
    "- Conclusions (1/2 page)\n",
    "- References\n",
    "- Acknowledgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ipub": {
     "init_cell": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a) Producing the data for the one-dimensional Ising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### define Ising model aprams\n",
    "# system size\n",
    "L=40\n",
    "\n",
    "# create 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(10000,L))\n",
    "\n",
    "def ising_energies(states,L):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    J=np.zeros((L,L),)\n",
    "    for i in range(L):\n",
    "        J[i,(i+1)%L]-=1.0\n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "# build final data set\n",
    "Data=[states,energies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of samples\n",
    "n_samples=400\n",
    "# define train and test data sets\n",
    "X_train=Data[0][:n_samples]\n",
    "Y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])\n",
    "X_test=Data[0][n_samples:3*n_samples//2]\n",
    "Y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) Estimating the coupling constant of the one-dimensional Ising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Code example",
      "format": {},
      "label": "code:example_code",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "OLS, Ridge, Lasso",
      "label": "fig:OLSRidgeLasso"
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# define error lists\n",
    "train_errors_leastsq = []\n",
    "test_errors_leastsq = []\n",
    "train_MSE_leastsq = []\n",
    "test_MSE_leastsq = []\n",
    "train_bias_leastsq = []\n",
    "test_bias_leastsq = []\n",
    "train_var_leastsq = []\n",
    "test_var_leastsq = []\n",
    "\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "train_MSE_ridge = []\n",
    "test_MSE_ridge = []\n",
    "train_bias_ridge = []\n",
    "test_bias_ridge = []\n",
    "train_var_ridge = []\n",
    "test_var_ridge = []\n",
    "\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "train_MSE_lasso = []\n",
    "test_MSE_lasso = []\n",
    "train_bias_lasso = []\n",
    "test_bias_lasso = []\n",
    "train_var_lasso = []\n",
    "test_var_lasso = []\n",
    "\n",
    "# set regularisation strength values\n",
    "lmbdas = np.logspace(-4, 5, 10)\n",
    "\n",
    "#Initialize coeffficients for OLS, ridge regression and Lasso\n",
    "coefs_leastsq = []\n",
    "coefs_ridge = []\n",
    "coefs_lasso=[]\n",
    "# set up Lasso Regression model\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "for _,lmbda in enumerate(lmbdas):\n",
    "    ### ordinary least squares\n",
    "    xb = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "    #fit model/singularity :\n",
    "    beta_ols = np.linalg.pinv(xb.T @ xb) @ xb.T @ Y_train \n",
    "    coefs_leastsq.append(beta_ols) # store weights\n",
    "    \n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    fitted_train = xb  @ beta_ols\n",
    "    xb_test = np.c_[np.ones((X_test.shape[0],1)),X_test]\n",
    "    \n",
    "    fitted_test = xb_test @ beta_ols\n",
    "    R2_train = 1 - np.sum( (fitted_train - Y_train)**2 )/np.sum( (Y_train - np.mean(Y_train))**2 )\n",
    "    R2_test = 1 - np.sum( (fitted_test - Y_test)**2 )/np.sum((Y_test - np.mean(Y_test))**2)\n",
    "    MSE_train = np.sum((fitted_train - Y_train)**2)/len(Y_train)\n",
    "    MSE_test = np.sum((fitted_test - Y_test)**2)/len(Y_test)\n",
    "    var_train = np.sum((fitted_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    var_test = np.sum((fitted_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    bias_train = np.sum((Y_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    bias_test = np.sum((Y_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    train_errors_leastsq.append(R2_train)\n",
    "    test_errors_leastsq.append(R2_test)\n",
    "    train_MSE_leastsq.append(MSE_train)\n",
    "    test_MSE_leastsq.append(MSE_test)\n",
    "    train_bias_leastsq.append(bias_train)\n",
    "    test_bias_leastsq.append(bias_test)\n",
    "    train_var_leastsq.append(var_train)\n",
    "    test_var_leastsq.append(var_test)\n",
    "\n",
    "    \n",
    "    ### apply Ridge regression\n",
    "    \n",
    "    I3 = np.eye(xb.shape[1]) \n",
    "    beta_ridge = (np.linalg.inv(xb.T @ xb + lmbda*I3) @ xb.T @ Y_train).flatten()\n",
    "  \n",
    "    coefs_ridge.append(beta_ridge[1:]) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    fitted_train = xb  @ beta_ridge\n",
    "    fitted_test = xb_test @ beta_ridge\n",
    "    R2_train = 1 - np.sum( (fitted_train - Y_train)**2 )/np.sum( (Y_train - np.mean(Y_train))**2 )\n",
    "    R2_test = 1 - np.sum( (fitted_test - Y_test)**2 )/np.sum((Y_test - np.mean(Y_test))**2)\n",
    "    MSE_train = np.sum((fitted_train - Y_train)**2)/len(Y_train)\n",
    "    MSE_test = np.sum((fitted_test - Y_test)**2)/len(Y_test)\n",
    "    var_train = np.sum((fitted_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    var_test = np.sum((fitted_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    bias_train = np.sum((Y_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    bias_test = np.sum((Y_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    train_errors_ridge.append(R2_train)\n",
    "    test_errors_ridge.append(R2_test)\n",
    "    train_MSE_ridge.append(MSE_train)\n",
    "    test_MSE_ridge.append(MSE_test)\n",
    "    train_bias_ridge.append(bias_train)\n",
    "    test_bias_ridge.append(bias_test)\n",
    "    train_var_ridge.append(var_train)\n",
    "    test_var_ridge.append(var_test)\n",
    "\n",
    "    \n",
    "    ### apply Lasso regression\n",
    "    lasso.set_params(alpha=lmbda) # set regularisation parameter\n",
    "    lasso.fit(X_train, Y_train) # fit model\n",
    "    \n",
    "    coefs_lasso.append(lasso.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    \n",
    "    test_pred = np.array(lasso.predict(X_test))\n",
    "    train_pred = np.array(lasso.predict(X_train))\n",
    "    \n",
    "    var_train = np.sum((train_pred - np.mean(train_pred))**2)/len(Y_train)\n",
    "    var_test = np.sum((test_pred - np.mean(test_pred))**2)/len(Y_test)\n",
    "    bias_train = np.sum((Y_train - np.mean(train_pred))**2)/len(Y_train)\n",
    "    bias_test = np.sum((Y_test - np.mean(test_pred))**2)/len(Y_test)\n",
    "    \n",
    "    train_errors_lasso.append(lasso.score(X_train, Y_train))\n",
    "    test_errors_lasso.append(lasso.score(X_test,Y_test))\n",
    "    \n",
    "    train_MSE_lasso.append(sklearn.metrics.mean_squared_error(Y_train, train_pred))\n",
    "    test_MSE_lasso.append(sklearn.metrics.mean_squared_error(Y_test, test_pred))\n",
    "    train_bias_lasso.append(bias_train)\n",
    "    test_bias_lasso.append(bias_test)\n",
    "    train_var_lasso.append(var_train)\n",
    "    test_var_lasso.append(var_test)\n",
    "    \n",
    "    ### plot Ising interaction J\n",
    "    J_leastsq=np.array(beta_ols[1:]).reshape((L,L))\n",
    "    J_ridge=np.array(beta_ridge[1:]).reshape((L,L))\n",
    "    J_lasso=np.array(lasso.coef_).reshape((L,L))\n",
    "\n",
    "    cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "\n",
    "    fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "    \n",
    "    axarr[0].imshow(J_leastsq,**cmap_args)\n",
    "    axarr[0].set_title('$\\\\mathrm{OLS}$',fontsize=16)\n",
    "    axarr[0].tick_params(labelsize=16)\n",
    "    \n",
    "    axarr[1].imshow(J_ridge,**cmap_args)\n",
    "    axarr[1].set_title('$\\\\mathrm{Ridge},\\ \\\\lambda=%.4f$' %(lmbda),fontsize=16)\n",
    "    axarr[1].tick_params(labelsize=16)\n",
    "    \n",
    "    im = axarr[2].imshow(J_lasso,**cmap_args)\n",
    "    axarr[2].set_title('$\\\\mathrm{LASSO},\\ \\\\lambda=%.4f$' %(lmbda),fontsize=16)\n",
    "    axarr[2].tick_params(labelsize=16)\n",
    "    \n",
    "    divider = make_axes_locatable(axarr[2])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.5)\n",
    "    cbar=fig.colorbar(im, cax=cax)\n",
    "    \n",
    "    cbar.ax.set_yticklabels(np.arange(-1.0, 1.0+0.25, 0.25),fontsize=14)\n",
    "    cbar.set_label('$J_{i,j}$',labelpad=-40, y=1.12,fontsize=16,rotation=0)\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    plt.show();\n",
    "    #fig.savefig('lasso_ridge'+str()+'.png') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance",
      "format": {},
      "label": "code:performance",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance",
      "label": "fig:performance"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_errors_leastsq, 'b',label='Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_errors_leastsq,'--b',label='Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_errors_ridge,'r',label='Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_errors_ridge,'--r',label='Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_errors_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_errors_lasso, '--g',label='Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Performance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the results\n",
    "\n",
    "Let us make a few remarks: (i) the (inverse, see [Scikit documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)) regularization parameter $\\lambda$ affects the Ridge and LASSO regressions at scales, separated by a few orders of magnitude. Notice that this is different for the data considered in Notebook 3 __Section VI: Linear Regression (Diabetes)__. Therefore, it is considered good practice to always check the performance for the given model and data with $\\lambda$. (ii) at $\\lambda\\to 0$ and $\\lambda\\to\\infty$, all three models overfit the data, as can be seen from the deviation of the test errors from unity (dashed lines), while the training curves stay at unity. (iii) While the OLS and Ridge regression test curves are monotonic, the LASSO test curve is not -- suggesting the optimal LASSO regularization parameter is $\\lambda\\approx 10^{-2}$. At this sweet spot, the Ising interaction weights ${\\bf J}$ contain only nearest-neighbor terms (as did the model the data was generated from).\n",
    "\n",
    "Gauge degrees of freedom: recall that the uniform nearest-neighbor interactions strength $J_{j,k}=J$ which we used to generate the data was set to unity, $J=1$. Moreover, $J_{j,k}$ was NOT defined to be symmetric (we only used the $J_{j,j+1}$ but never the $J_{j,j-1}$ elements). The colorbar on the matrix elements plot above suggest that the OLS and Ridge regression learn uniform symmetric weights $J=-0.5$. There is no mystery since this amounts to taking into account both the $J_{j,j+1}$ and the $J_{j,j-1}$ terms, and the weights are distributed symmetrically between them. LASSO, on the other hand, can break this symmetry (see matrix elements plots for $\\lambda=0.001$ and $\\lambda=0.01$). Thus, we see how different regularization schemes can lead to learning equivalent models but in different gauges. Any information we have about the symmetry of the unknown model that generated the data has to be reflected in the definition of the model and the regularization chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance MSE",
      "format": {},
      "label": "code:performanceMSE",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance MSE",
      "label": "fig:performanceMSE"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_MSE_leastsq, 'b',label='Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_MSE_leastsq,'--b',label='Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_MSE_ridge,'r',label='Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_MSE_ridge,'--r',label='Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_MSE_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_MSE_lasso, '--g',label='Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Performance-MSE',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance bias/variance",
      "format": {},
      "label": "code:performancebiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance bias/variance",
      "label": "fig:performancebiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance OlS bias/variance",
      "format": {},
      "label": "code:performanceOLSbiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance OLS bias/variance",
      "label": "fig:performanceOLSbiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance Ridge bias/variance",
      "format": {},
      "label": "code:performanceRidgebiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance Ridge bias/variance",
      "label": "fig:performanceRidgebiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "#plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "#plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance Lasso bias/variance",
      "format": {},
      "label": "code:performanceLassobiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance Lasso bias/variance",
      "label": "fig:performanceLassobiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "#plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "#plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c) Determine the phase of the two-dimensional Ising model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "",
      "format": {},
      "label": "code:example_code",
      "placement": "H",
      "widefigure": false
     }
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1) # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "",
      "format": {},
      "label": "code:example_code",
      "placement": "H",
      "widefigure": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (65000, 1600)\n",
      "Y_train shape: (65000,)\n",
      "\n",
      "65000 train samples\n",
      "30000 critical samples\n",
      "65000 test samples\n"
     ]
    }
   ],
   "source": [
    "##### prepare training and test data sets\n",
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "# path to data directory\n",
    "path_to_data=os.path.expanduser('.')+'/data/'\n",
    "\n",
    "# load data\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "data = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "#X_ordered[np.where(X_ordered==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "#X_critical[np.where(X_critical==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "#X_disordered[np.where(X_disordered==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Plotting some states",
      "format": {},
      "label": "code:somestates",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Plotting some Ising states: ordered phase, critical region, disordered phase",
      "label": "fig:somestates"
     }
    }
   },
   "outputs": [],
   "source": [
    "##### plot a few Ising states\n",
    "\n",
    "# set colourbar map\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "# plot states\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
    "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Accuracy",
      "format": {},
      "label": "code:accuracy",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Accuracy with respect to regularisation strength",
      "label": "fig:accuraryregularisation"
     }
    }
   },
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# define regularisation parameter\n",
    "lmbdas=np.logspace(-5,5,11)\n",
    "\n",
    "# preallocate data\n",
    "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "# loop over regularisation strength\n",
    "for i,lmbda in enumerate(lmbdas):\n",
    "\n",
    "    # define logistic regressor\n",
    "    logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5)\n",
    "\n",
    "    # fit training data\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy[i]=logreg.score(X_train,Y_train)\n",
    "    test_accuracy[i]=logreg.score(X_test,Y_test)\n",
    "    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('accuracy: train, test, critical')\n",
    "    print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
    "\n",
    "    # define SGD-based logistic regression\n",
    "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
    "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
    "\n",
    "    # fit training data\n",
    "    logreg_SGD.fit(X_train,Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
    "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
    "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
    "\n",
    "    print('finished computing %i/11 iterations' %(i+1))\n",
    "\n",
    "# plot accuracy against regularisation strength\n",
    "plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
    "plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
    "plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
    "\n",
    "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\n",
    "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\n",
    "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label='SGD critical')\n",
    "\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "lr = 0.5\n",
    "epochs = 300\n",
    "weights= logisRegresANA.logistic_reg(X_train, Y_train, epochs, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights=logisRegresANA.stocGradAscentA(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights = logisRegresANA.steepest_descent_auto(X_train, Y_train, alpha =0.001)\n",
    "error_train = logisRegresANA.simptest(weights, X_train, Y_train)\n",
    "error_test = logisRegresANA.simptest(weights, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights = logisRegresANA.logistic_reg(X_train, Y_train, epochs= 100, lr=0.001)\n",
    "error_train = logisRegresANA.simptest(weights, X_train, Y_train)\n",
    "error_test = logisRegresANA.simptest(weights, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights = logisRegresANA.gradDscent(X_train, Y_train, alpha= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logisRegresANA\n",
    "importlib.reload(logisRegresANA)\n",
    "\n",
    "weights2 = logisRegresANA.sgd(X_train, Y_train)\n",
    "print(weights2)\n",
    "weights2 = weights2.flatten()\n",
    "error_train = logisRegresANA.simptest(weights2, X_train, Y_train)\n",
    "error_test = logisRegresANA.simptest(weights2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d) Regression analysis of the one-dimensional Ising model using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e) Classifying the Ising model phase using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch  [[-1 -1 -1 ..., -1 -1  1]\n",
      " [-1 -1 -1 ..., -1 -1  0]\n",
      " [ 1  1  1 ...,  1  1  1]\n",
      " ..., \n",
      " [ 1  1  1 ..., -1 -1  0]\n",
      " [-1 -1 -1 ..., -1 -1  1]\n",
      " [ 1  1 -1 ...,  1 -1  0]]\n",
      "nabla_w from weights -shape  (2,)\n",
      "nabla_b from weights -shape  (2,)\n",
      "x  [-1 -1 -1 ..., -1 -1 -1]\n",
      "y  1\n",
      "activations:  [array([-1, -1, -1, ..., -1, -1, -1])]\n",
      "f 0 w in loop [[ 0.50892274 -0.28898022 -1.20827711 ..., -0.456469   -0.65965346\n",
      "   0.10241962]\n",
      " [ 2.3897599   1.29156472  1.27318914 ..., -0.55426639 -0.94425406\n",
      "  -0.90373304]\n",
      " [ 0.97948496 -0.55957186 -0.1711543  ...,  0.08315642 -0.76727449\n",
      "  -0.1474379 ]\n",
      " ..., \n",
      " [-0.75724226 -0.53077548 -1.08823808 ..., -0.28865485  0.48977372\n",
      "   1.44011092]\n",
      " [-0.01027559  0.83852329 -0.11277865 ..., -0.85940721  0.45309176\n",
      "  -0.42789696]\n",
      " [ 0.26413263  1.05422235 -0.36335701 ..., -1.30188984 -0.40503143\n",
      "  -0.68552322]]\n",
      "activation :  [-1 -1 -1 ..., -1 -1 -1]\n",
      "w_a =  numpy.dot(w , activation):  [-13.49316819 -39.12589526 -10.07647132  37.48403681 -26.31504817\n",
      "  15.09851548 -13.01447302  18.43597532 -29.68447902  -0.04488525]\n",
      "b : [-0.9073413  -1.36768501 -0.35959421 -0.89205373 -1.35507506 -0.08539942\n",
      " -1.56692176  0.74175751  0.30745289 -0.95706228]\n",
      "z = w_a + b :  [-14.40050949 -40.49358026 -10.43606554  36.59198309 -27.67012324\n",
      "  15.01311605 -14.58139478  19.17773283 -29.37702613  -1.00194753]\n",
      "f 1 w in loop [[ 0.74683728 -1.36658155  1.73000956 -0.99789933 -0.8230627  -0.840913\n",
      "  -0.48278465  0.37804188  0.20443838  0.55033394]]\n",
      "activation :  [  5.57106144e-07   2.59335242e-18   2.93536138e-05   1.00000000e+00\n",
      "   9.61652500e-13   9.99999698e-01   4.64922440e-07   9.99999995e-01\n",
      "   1.74469556e-13   2.68558686e-01]\n",
      "w_a =  numpy.dot(w , activation):  [-1.31292226]\n",
      "b : [ 0.6802191]\n",
      "z = w_a + b :  [-0.63270316]\n",
      "ACTIVATIONS :  [-1 -1 -1 ..., -1 -1 -1]\n",
      "[  5.57106144e-07   2.59335242e-18   2.93536138e-05   1.00000000e+00\n",
      "   9.61652500e-13   9.99999698e-01   4.64922440e-07   9.99999995e-01\n",
      "   1.74469556e-13   2.68558686e-01]\n",
      "[ 0.34689786]\n",
      "in last layer, y  1 activations[-1 ] [ 0.34689786]\n",
      "zs stored : [array([-14.40050949, -40.49358026, -10.43606554,  36.59198309,\n",
      "       -27.67012324,  15.01311605, -14.58139478,  19.17773283,\n",
      "       -29.37702613,  -1.00194753]), array([-0.63270316])]\n",
      "DELTA :  [-0.65310214]\n",
      "in backprop: numpy.array(activations[-2]).T  [  5.57106144e-07   2.59335242e-18   2.93536138e-05   1.00000000e+00\n",
      "   9.61652500e-13   9.99999698e-01   4.64922440e-07   9.99999995e-01\n",
      "   1.74469556e-13   2.68558686e-01]\n",
      "in backprop: nabla_b_backprop[-1]  [-0.65310214]\n",
      "in backprop: nabla_w_backprop[-1]  [ -3.63847217e-07  -1.69372402e-18  -1.91709081e-05  -6.53102143e-01\n",
      "  -6.28057308e-13  -6.53101946e-01  -3.03641842e-07  -6.53102140e-01\n",
      "  -1.13946441e-13  -1.75396253e-01]\n",
      "ENTRA NESTE LOOP, k =  1\n",
      "sp  [  5.57105834e-07   2.59335242e-18   2.93527522e-05   2.22044605e-16\n",
      "   9.61652500e-13   3.01916105e-07   4.64922224e-07   4.69047088e-09\n",
      "   1.74469556e-13   1.96434918e-01]\n",
      "numpy.array(weights[-1- (k-1)]).T  [[ 0.74683728]\n",
      " [-1.36658155]\n",
      " [ 1.73000956]\n",
      " [-0.99789933]\n",
      " [-0.8230627 ]\n",
      " [-0.840913  ]\n",
      " [-0.48278465]\n",
      " [ 0.37804188]\n",
      " [ 0.20443838]\n",
      " [ 0.55033394]]\n",
      "delta  -0.653102142795\n",
      "delta in loop k [ -2.71734513e-07   2.31461199e-18  -3.31648807e-05   1.44713173e-16\n",
      "   5.16930545e-13   1.65812954e-07   1.46593551e-07  -1.15807709e-09\n",
      "  -2.32950262e-14  -7.06034783e-02]\n",
      "testyy [-1 -1 -1 ..., -1 -1 -1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (1600,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0374de8f71b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                   \u001b[0mvalidation_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                              epochs= 30, mini_batch_size = 10, lr= 0.5, C='ce')\n\u001b[0m",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mneuralnetwork\u001b[0;34m(sizes, X_train, Y_train, validation_x, validation_y, verbose, epochs, mini_batch_size, lr, C)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m#print('initial weights ', weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     biases, weights = SGD(X_train, Y_train, epochs, mini_batch_size, lr, C, sizes, num_layers, \n\u001b[0;32m--> 140\u001b[0;31m         biases, weights, verbose, validation_x, validation_y)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m def SGD(X_train, Y_train, epochs, mini_batch_size, lr, C, sizes, num_layers,\n",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X_train, Y_train, epochs, mini_batch_size, lr, C, sizes, num_layers, biases, weights, verbose, validation_x, validation_y)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m#feed-forward (and back) all mini_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(minibatch, lr, C, sizes, num_layers, biases, weights)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m#backpropagation for each observation in mini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;31m#print(delta_nabla_b.shape, 'delta_nabla_b ', delta_nabla_b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'delta_nabla_w '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(x, y, C, sizes, num_layers, biases, weights)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mtestyy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testyy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestyy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mnabla_w_backprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtestyy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnabla_b_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w_backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (1600,) "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import logisRegresANA\n",
    "importlib.reload(logisRegresANA)\n",
    "\n",
    "in_layer = X_train.shape[1] #number of neurons in the input layer\n",
    "\n",
    "if (len(Y_train.shape)==1): \n",
    "    out_layer = 1   #number of neurons in the output layer\n",
    "else: out_layer = Y_train.shape[1]\n",
    "biasesnn, weightsnn= logisRegresANA.neuralnetwork([in_layer, 10, out_layer], X_train, Y_train,\n",
    "                                                  validation_x=X_test, validation_y=Y_test,\n",
    "                                                  verbose=True,\n",
    "                             epochs= 30, mini_batch_size = 10, lr= 0.5, C='ce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Part f) Critical evaluation of the various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ipub": {
     "ignore": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-1332f8c86a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This cell needs to be executed twice (for references)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nbpublish -f latex_ipypublish_all -pdf Untitled.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nbpublish -f latex_ipypublish_all -pdf Untitled.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2413\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pexpect factory incomplete>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptyproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_spawnpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Use internal fork_pty, for Solaris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/workshop2/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmaster_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Establish a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "#This cell needs to be executed twice (for references)\n",
    "!nbpublish -f latex_ipypublish_all -pdf Untitled.ipynb\n",
    "!nbpublish -f latex_ipypublish_all -pdf Untitled.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "ipub": {
   "bibliography": "ana.bib",
   "titlepage": {
    "author": "Ana Costa",
    "email": "authors@email.com",
    "institution": [
     "UiO"
    ],
    "logo": "UiO_Segl_A.png",
    "subtitle": "Logistic Regression and Neural Network",
    "supervisors": [
     ""
    ],
    "tagline": "",
    "title": "Logistic Regression and Neural Network"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T10:09:55.192497Z",
     "start_time": "2018-11-01T10:09:55.189708Z"
    },
    "ipub": {
     "ignore": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=10>Classification and Regression from linear and logistic regression to neural networks</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/anacost/project2-FYS-STK4155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General structure\n",
    "\n",
    "- Project title\n",
    "- Name, email, course title, date, group assistant\n",
    "- Abstract (1/2 page max)\n",
    "- Introduction (1 page)\n",
    "- Method\n",
    "    - Packages used\n",
    "    - Datasets (models and observations)\n",
    "    - Analysis method\n",
    "    - â€¦\n",
    "- Results\n",
    "- Discussion and outlook (1 page)\n",
    "- Conclusions (1/2 page)\n",
    "- References\n",
    "- Acknowledgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ipub": {
     "init_cell": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a) Producing the data for the one-dimensional Ising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### define Ising model aprams\n",
    "# system size\n",
    "L=40\n",
    "\n",
    "# create 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(10000,L))\n",
    "\n",
    "def ising_energies(states,L):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    J=np.zeros((L,L),)\n",
    "    for i in range(L):\n",
    "        J[i,(i+1)%L]-=1.0\n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "# build final data set\n",
    "Data=[states,energies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of samples\n",
    "n_samples=400\n",
    "# define train and test data sets\n",
    "X_train=Data[0][:n_samples]\n",
    "Y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])\n",
    "X_test=Data[0][n_samples:3*n_samples//2]\n",
    "Y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) Estimating the coupling constant of the one-dimensional Ising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Code example",
      "format": {},
      "label": "code:example_code",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "OLS, Ridge, Lasso",
      "label": "fig:OLSRidgeLasso"
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# define error lists\n",
    "train_errors_leastsq = []\n",
    "test_errors_leastsq = []\n",
    "train_MSE_leastsq = []\n",
    "test_MSE_leastsq = []\n",
    "train_bias_leastsq = []\n",
    "test_bias_leastsq = []\n",
    "train_var_leastsq = []\n",
    "test_var_leastsq = []\n",
    "\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "train_MSE_ridge = []\n",
    "test_MSE_ridge = []\n",
    "train_bias_ridge = []\n",
    "test_bias_ridge = []\n",
    "train_var_ridge = []\n",
    "test_var_ridge = []\n",
    "\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "train_MSE_lasso = []\n",
    "test_MSE_lasso = []\n",
    "train_bias_lasso = []\n",
    "test_bias_lasso = []\n",
    "train_var_lasso = []\n",
    "test_var_lasso = []\n",
    "\n",
    "# set regularisation strength values\n",
    "lmbdas = np.logspace(-4, 5, 10)\n",
    "\n",
    "#Initialize coeffficients for OLS, ridge regression and Lasso\n",
    "coefs_leastsq = []\n",
    "coefs_ridge = []\n",
    "coefs_lasso=[]\n",
    "# set up Lasso Regression model\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "for _,lmbda in enumerate(lmbdas):\n",
    "    ### ordinary least squares\n",
    "    xb = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "    #fit model/singularity :\n",
    "    beta_ols = np.linalg.pinv(xb.T @ xb) @ xb.T @ Y_train \n",
    "    coefs_leastsq.append(beta_ols) # store weights\n",
    "    \n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    fitted_train = xb  @ beta_ols\n",
    "    xb_test = np.c_[np.ones((X_test.shape[0],1)),X_test]\n",
    "    \n",
    "    fitted_test = xb_test @ beta_ols\n",
    "    R2_train = 1 - np.sum( (fitted_train - Y_train)**2 )/np.sum( (Y_train - np.mean(Y_train))**2 )\n",
    "    R2_test = 1 - np.sum( (fitted_test - Y_test)**2 )/np.sum((Y_test - np.mean(Y_test))**2)\n",
    "    MSE_train = np.sum((fitted_train - Y_train)**2)/len(Y_train)\n",
    "    MSE_test = np.sum((fitted_test - Y_test)**2)/len(Y_test)\n",
    "    var_train = np.sum((fitted_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    var_test = np.sum((fitted_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    bias_train = np.sum((Y_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    bias_test = np.sum((Y_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    train_errors_leastsq.append(R2_train)\n",
    "    test_errors_leastsq.append(R2_test)\n",
    "    train_MSE_leastsq.append(MSE_train)\n",
    "    test_MSE_leastsq.append(MSE_test)\n",
    "    train_bias_leastsq.append(bias_train)\n",
    "    test_bias_leastsq.append(bias_test)\n",
    "    train_var_leastsq.append(var_train)\n",
    "    test_var_leastsq.append(var_test)\n",
    "\n",
    "    \n",
    "    ### apply Ridge regression\n",
    "    \n",
    "    I3 = np.eye(xb.shape[1]) \n",
    "    beta_ridge = (np.linalg.inv(xb.T @ xb + lmbda*I3) @ xb.T @ Y_train).flatten()\n",
    "  \n",
    "    coefs_ridge.append(beta_ridge[1:]) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    fitted_train = xb  @ beta_ridge\n",
    "    fitted_test = xb_test @ beta_ridge\n",
    "    R2_train = 1 - np.sum( (fitted_train - Y_train)**2 )/np.sum( (Y_train - np.mean(Y_train))**2 )\n",
    "    R2_test = 1 - np.sum( (fitted_test - Y_test)**2 )/np.sum((Y_test - np.mean(Y_test))**2)\n",
    "    MSE_train = np.sum((fitted_train - Y_train)**2)/len(Y_train)\n",
    "    MSE_test = np.sum((fitted_test - Y_test)**2)/len(Y_test)\n",
    "    var_train = np.sum((fitted_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    var_test = np.sum((fitted_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    bias_train = np.sum((Y_train - np.mean(fitted_train))**2)/len(Y_train)\n",
    "    bias_test = np.sum((Y_test - np.mean(fitted_test))**2)/len(Y_test)\n",
    "    train_errors_ridge.append(R2_train)\n",
    "    test_errors_ridge.append(R2_test)\n",
    "    train_MSE_ridge.append(MSE_train)\n",
    "    test_MSE_ridge.append(MSE_test)\n",
    "    train_bias_ridge.append(bias_train)\n",
    "    test_bias_ridge.append(bias_test)\n",
    "    train_var_ridge.append(var_train)\n",
    "    test_var_ridge.append(var_test)\n",
    "\n",
    "    \n",
    "    ### apply Lasso regression\n",
    "    lasso.set_params(alpha=lmbda) # set regularisation parameter\n",
    "    lasso.fit(X_train, Y_train) # fit model\n",
    "    \n",
    "    coefs_lasso.append(lasso.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    \n",
    "    test_pred = np.array(lasso.predict(X_test))\n",
    "    train_pred = np.array(lasso.predict(X_train))\n",
    "    \n",
    "    var_train = np.sum((train_pred - np.mean(train_pred))**2)/len(Y_train)\n",
    "    var_test = np.sum((test_pred - np.mean(test_pred))**2)/len(Y_test)\n",
    "    bias_train = np.sum((Y_train - np.mean(train_pred))**2)/len(Y_train)\n",
    "    bias_test = np.sum((Y_test - np.mean(test_pred))**2)/len(Y_test)\n",
    "    \n",
    "    train_errors_lasso.append(lasso.score(X_train, Y_train))\n",
    "    test_errors_lasso.append(lasso.score(X_test,Y_test))\n",
    "    \n",
    "    train_MSE_lasso.append(sklearn.metrics.mean_squared_error(Y_train, train_pred))\n",
    "    test_MSE_lasso.append(sklearn.metrics.mean_squared_error(Y_test, test_pred))\n",
    "    train_bias_lasso.append(bias_train)\n",
    "    test_bias_lasso.append(bias_test)\n",
    "    train_var_lasso.append(var_train)\n",
    "    test_var_lasso.append(var_test)\n",
    "    \n",
    "    ### plot Ising interaction J\n",
    "    J_leastsq=np.array(beta_ols[1:]).reshape((L,L))\n",
    "    J_ridge=np.array(beta_ridge[1:]).reshape((L,L))\n",
    "    J_lasso=np.array(lasso.coef_).reshape((L,L))\n",
    "\n",
    "    cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "\n",
    "    fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "    \n",
    "    axarr[0].imshow(J_leastsq,**cmap_args)\n",
    "    axarr[0].set_title('$\\\\mathrm{OLS}$',fontsize=16)\n",
    "    axarr[0].tick_params(labelsize=16)\n",
    "    \n",
    "    axarr[1].imshow(J_ridge,**cmap_args)\n",
    "    axarr[1].set_title('$\\\\mathrm{Ridge},\\ \\\\lambda=%.4f$' %(lmbda),fontsize=16)\n",
    "    axarr[1].tick_params(labelsize=16)\n",
    "    \n",
    "    im = axarr[2].imshow(J_lasso,**cmap_args)\n",
    "    axarr[2].set_title('$\\\\mathrm{LASSO},\\ \\\\lambda=%.4f$' %(lmbda),fontsize=16)\n",
    "    axarr[2].tick_params(labelsize=16)\n",
    "    \n",
    "    divider = make_axes_locatable(axarr[2])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.5)\n",
    "    cbar=fig.colorbar(im, cax=cax)\n",
    "    \n",
    "    cbar.ax.set_yticklabels(np.arange(-1.0, 1.0+0.25, 0.25),fontsize=14)\n",
    "    cbar.set_label('$J_{i,j}$',labelpad=-40, y=1.12,fontsize=16,rotation=0)\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    plt.show();\n",
    "    #fig.savefig('lasso_ridge'+str()+'.png') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance",
      "format": {},
      "label": "code:performance",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance",
      "label": "fig:performance"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_errors_leastsq, 'b',label='Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_errors_leastsq,'--b',label='Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_errors_ridge,'r',label='Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_errors_ridge,'--r',label='Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_errors_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_errors_lasso, '--g',label='Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Performance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the results\n",
    "\n",
    "Let us make a few remarks: (i) the (inverse, see [Scikit documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)) regularization parameter $\\lambda$ affects the Ridge and LASSO regressions at scales, separated by a few orders of magnitude. Notice that this is different for the data considered in Notebook 3 __Section VI: Linear Regression (Diabetes)__. Therefore, it is considered good practice to always check the performance for the given model and data with $\\lambda$. (ii) at $\\lambda\\to 0$ and $\\lambda\\to\\infty$, all three models overfit the data, as can be seen from the deviation of the test errors from unity (dashed lines), while the training curves stay at unity. (iii) While the OLS and Ridge regression test curves are monotonic, the LASSO test curve is not -- suggesting the optimal LASSO regularization parameter is $\\lambda\\approx 10^{-2}$. At this sweet spot, the Ising interaction weights ${\\bf J}$ contain only nearest-neighbor terms (as did the model the data was generated from).\n",
    "\n",
    "Gauge degrees of freedom: recall that the uniform nearest-neighbor interactions strength $J_{j,k}=J$ which we used to generate the data was set to unity, $J=1$. Moreover, $J_{j,k}$ was NOT defined to be symmetric (we only used the $J_{j,j+1}$ but never the $J_{j,j-1}$ elements). The colorbar on the matrix elements plot above suggest that the OLS and Ridge regression learn uniform symmetric weights $J=-0.5$. There is no mystery since this amounts to taking into account both the $J_{j,j+1}$ and the $J_{j,j-1}$ terms, and the weights are distributed symmetrically between them. LASSO, on the other hand, can break this symmetry (see matrix elements plots for $\\lambda=0.001$ and $\\lambda=0.01$). Thus, we see how different regularization schemes can lead to learning equivalent models but in different gauges. Any information we have about the symmetry of the unknown model that generated the data has to be reflected in the definition of the model and the regularization chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance MSE",
      "format": {},
      "label": "code:performanceMSE",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance MSE",
      "label": "fig:performanceMSE"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_MSE_leastsq, 'b',label='Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_MSE_leastsq,'--b',label='Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_MSE_ridge,'r',label='Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_MSE_ridge,'--r',label='Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_MSE_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_MSE_lasso, '--g',label='Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Performance-MSE',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance bias/variance",
      "format": {},
      "label": "code:performancebiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance bias/variance",
      "label": "fig:performancebiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance OlS bias/variance",
      "format": {},
      "label": "code:performanceOLSbiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance OLS bias/variance",
      "label": "fig:performanceOLSbiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance Ridge bias/variance",
      "format": {},
      "label": "code:performanceRidgebiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance Ridge bias/variance",
      "label": "fig:performanceRidgebiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "#plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "#plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "#plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Performance Lasso bias/variance",
      "format": {},
      "label": "code:performanceLassobiasvar",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Performance Lasso bias/variance",
      "label": "fig:performanceLassobiasvar"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot our bias-variance on both the training and test data\n",
    "#plt.semilogx(lmbdas, train_bias_leastsq, 'b',label='Bias-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_bias_leastsq,'--b',label='Bias-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_bias_ridge,'r',label='Bias-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_bias_ridge,'--r',label='Bias-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_bias_lasso, 'g',label='Bias-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_bias_lasso, '--g',label='Bias-Test (LASSO)')\n",
    "\n",
    "#plt.semilogx(lmbdas, train_var_leastsq, ':b',label='Variance-Train (OLS)')\n",
    "#plt.semilogx(lmbdas, test_var_leastsq,'.b',label='Variance-Test (OLS)')\n",
    "#plt.semilogx(lmbdas, train_var_ridge,':r',label='Variance-Train (Ridge)',linewidth=1)\n",
    "#plt.semilogx(lmbdas, test_var_ridge,'.r',label='Variance-Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_var_lasso, ':g',label='Variance-Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_var_lasso, '.g',label='Variance-Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Bias-Variance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c) Determine the phase of the two-dimensional Ising model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "",
      "format": {},
      "label": "code:example_code",
      "placement": "H",
      "widefigure": false
     }
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1) # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "",
      "format": {},
      "label": "code:example_code",
      "placement": "H",
      "widefigure": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (65000, 1600)\n",
      "Y_train shape: (65000,)\n",
      "\n",
      "65000 train samples\n",
      "30000 critical samples\n",
      "65000 test samples\n"
     ]
    }
   ],
   "source": [
    "##### prepare training and test data sets\n",
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "# path to data directory\n",
    "path_to_data=os.path.expanduser('.')+'/data/'\n",
    "\n",
    "# load data\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "data = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "#X_ordered[np.where(X_ordered==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "#X_critical[np.where(X_critical==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "#X_disordered[np.where(X_disordered==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Plotting some states",
      "format": {},
      "label": "code:somestates",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Plotting some Ising states: ordered phase, critical region, disordered phase",
      "label": "fig:somestates"
     }
    }
   },
   "outputs": [],
   "source": [
    "##### plot a few Ising states\n",
    "\n",
    "# set colourbar map\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "# plot states\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
    "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ipub": {
     "code": {
      "asfloat": true,
      "caption": "Accuracy",
      "format": {},
      "label": "code:accuracy",
      "placement": "H",
      "widefigure": false
     },
     "figure": {
      "caption": "Accuracy with respect to regularisation strength",
      "label": "fig:accuraryregularisation"
     }
    }
   },
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# define regularisation parameter\n",
    "lmbdas=np.logspace(-5,5,11)\n",
    "\n",
    "# preallocate data\n",
    "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "# loop over regularisation strength\n",
    "for i,lmbda in enumerate(lmbdas):\n",
    "\n",
    "    # define logistic regressor\n",
    "    logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5)\n",
    "\n",
    "    # fit training data\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy[i]=logreg.score(X_train,Y_train)\n",
    "    test_accuracy[i]=logreg.score(X_test,Y_test)\n",
    "    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('accuracy: train, test, critical')\n",
    "    print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
    "\n",
    "    # define SGD-based logistic regression\n",
    "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
    "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
    "\n",
    "    # fit training data\n",
    "    logreg_SGD.fit(X_train,Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
    "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
    "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
    "\n",
    "    print('finished computing %i/11 iterations' %(i+1))\n",
    "\n",
    "# plot accuracy against regularisation strength\n",
    "plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
    "plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
    "plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
    "\n",
    "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\n",
    "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\n",
    "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label='SGD critical')\n",
    "\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "lr = 0.5\n",
    "epochs = 300\n",
    "weights= logisRegresANA.logistic_reg(X_train, Y_train, epochs, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights=logisRegresANA.stocGradAscentA(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights = logisRegresANA.steepest_descent_auto(X_train, Y_train, alpha =0.001)\n",
    "error_train = logisRegresANA.simptest(weights, X_train, Y_train)\n",
    "error_test = logisRegresANA.simptest(weights, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights = logisRegresANA.logistic_reg(X_train, Y_train, epochs= 100, lr=0.001)\n",
    "error_train = logisRegresANA.simptest(weights, X_train, Y_train)\n",
    "error_test = logisRegresANA.simptest(weights, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply logistic regression\n",
    "import logisRegresANA\n",
    "import importlib\n",
    "importlib.reload(logisRegresANA)\n",
    "weights = logisRegresANA.gradDscent(X_train, Y_train, alpha= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logisRegresANA\n",
    "importlib.reload(logisRegresANA)\n",
    "\n",
    "weights2 = logisRegresANA.sgd(X_train, Y_train)\n",
    "print(weights2)\n",
    "weights2 = weights2.flatten()\n",
    "error_train = logisRegresANA.simptest(weights2, X_train, Y_train)\n",
    "error_test = logisRegresANA.simptest(weights2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d) Regression analysis of the one-dimensional Ising model using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e) Classifying the Ising model phase using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch  [[-1 -1 -1 ..., -1 -1  1]\n",
      " [-1 -1 -1 ..., -1 -1  0]\n",
      " [ 1  1  1 ...,  1  1  1]\n",
      " ..., \n",
      " [ 1  1  1 ..., -1 -1  0]\n",
      " [-1 -1 -1 ..., -1 -1  1]\n",
      " [ 1  1 -1 ...,  1 -1  0]]\n",
      "nabla_w from weights -shape  (2,)\n",
      "nabla_b from weights -shape  (2,)\n",
      "x  [-1 -1 -1 ..., -1 -1 -1]\n",
      "y  1\n",
      "activations:  [array([-1, -1, -1, ..., -1, -1, -1])]\n",
      "f 0 w in loop [[ 0.50892274 -0.28898022 -1.20827711 ..., -0.456469   -0.65965346\n",
      "   0.10241962]\n",
      " [ 2.3897599   1.29156472  1.27318914 ..., -0.55426639 -0.94425406\n",
      "  -0.90373304]\n",
      " [ 0.97948496 -0.55957186 -0.1711543  ...,  0.08315642 -0.76727449\n",
      "  -0.1474379 ]\n",
      " ..., \n",
      " [-0.75724226 -0.53077548 -1.08823808 ..., -0.28865485  0.48977372\n",
      "   1.44011092]\n",
      " [-0.01027559  0.83852329 -0.11277865 ..., -0.85940721  0.45309176\n",
      "  -0.42789696]\n",
      " [ 0.26413263  1.05422235 -0.36335701 ..., -1.30188984 -0.40503143\n",
      "  -0.68552322]]\n",
      "activation :  [-1 -1 -1 ..., -1 -1 -1]\n",
      "w_a =  numpy.dot(w , activation):  [-13.49316819 -39.12589526 -10.07647132  37.48403681 -26.31504817\n",
      "  15.09851548 -13.01447302  18.43597532 -29.68447902  -0.04488525]\n",
      "b : [-0.9073413  -1.36768501 -0.35959421 -0.89205373 -1.35507506 -0.08539942\n",
      " -1.56692176  0.74175751  0.30745289 -0.95706228]\n",
      "z = w_a + b :  [-14.40050949 -40.49358026 -10.43606554  36.59198309 -27.67012324\n",
      "  15.01311605 -14.58139478  19.17773283 -29.37702613  -1.00194753]\n",
      "f 1 w in loop [[ 0.74683728 -1.36658155  1.73000956 -0.99789933 -0.8230627  -0.840913\n",
      "  -0.48278465  0.37804188  0.20443838  0.55033394]]\n",
      "activation :  [  5.57106144e-07   2.59335242e-18   2.93536138e-05   1.00000000e+00\n",
      "   9.61652500e-13   9.99999698e-01   4.64922440e-07   9.99999995e-01\n",
      "   1.74469556e-13   2.68558686e-01]\n",
      "w_a =  numpy.dot(w , activation):  [-1.31292226]\n",
      "b : [ 0.6802191]\n",
      "z = w_a + b :  [-0.63270316]\n",
      "ACTIVATIONS :  [-1 -1 -1 ..., -1 -1 -1]\n",
      "[  5.57106144e-07   2.59335242e-18   2.93536138e-05   1.00000000e+00\n",
      "   9.61652500e-13   9.99999698e-01   4.64922440e-07   9.99999995e-01\n",
      "   1.74469556e-13   2.68558686e-01]\n",
      "[ 0.34689786]\n",
      "in last layer, y  1 activations[-1 ] [ 0.34689786]\n",
      "zs stored : [array([-14.40050949, -40.49358026, -10.43606554,  36.59198309,\n",
      "       -27.67012324,  15.01311605, -14.58139478,  19.17773283,\n",
      "       -29.37702613,  -1.00194753]), array([-0.63270316])]\n",
      "DELTA :  [-0.65310214]\n",
      "in backprop: numpy.array(activations[-2]).T  [  5.57106144e-07   2.59335242e-18   2.93536138e-05   1.00000000e+00\n",
      "   9.61652500e-13   9.99999698e-01   4.64922440e-07   9.99999995e-01\n",
      "   1.74469556e-13   2.68558686e-01]\n",
      "in backprop: nabla_b_backprop[-1]  [-0.65310214]\n",
      "in backprop: nabla_w_backprop[-1]  [ -3.63847217e-07  -1.69372402e-18  -1.91709081e-05  -6.53102143e-01\n",
      "  -6.28057308e-13  -6.53101946e-01  -3.03641842e-07  -6.53102140e-01\n",
      "  -1.13946441e-13  -1.75396253e-01]\n",
      "ENTRA NESTE LOOP, k =  1\n",
      "sp  [  5.57105834e-07   2.59335242e-18   2.93527522e-05   2.22044605e-16\n",
      "   9.61652500e-13   3.01916105e-07   4.64922224e-07   4.69047088e-09\n",
      "   1.74469556e-13   1.96434918e-01]\n",
      "numpy.array(weights[-1- (k-1)]).T  [[ 0.74683728]\n",
      " [-1.36658155]\n",
      " [ 1.73000956]\n",
      " [-0.99789933]\n",
      " [-0.8230627 ]\n",
      " [-0.840913  ]\n",
      " [-0.48278465]\n",
      " [ 0.37804188]\n",
      " [ 0.20443838]\n",
      " [ 0.55033394]]\n",
      "delta  -0.653102142795\n",
      "delta in loop k [ -2.71734513e-07   2.31461199e-18  -3.31648807e-05   1.44713173e-16\n",
      "   5.16930545e-13   1.65812954e-07   1.46593551e-07  -1.15807709e-09\n",
      "  -2.32950262e-14  -7.06034783e-02]\n",
      "testyy [-1 -1 -1 ..., -1 -1 -1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (1600,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0374de8f71b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                   \u001b[0mvalidation_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                              epochs= 30, mini_batch_size = 10, lr= 0.5, C='ce')\n\u001b[0m",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mneuralnetwork\u001b[0;34m(sizes, X_train, Y_train, validation_x, validation_y, verbose, epochs, mini_batch_size, lr, C)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m#print('initial weights ', weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     biases, weights = SGD(X_train, Y_train, epochs, mini_batch_size, lr, C, sizes, num_layers, \n\u001b[0;32m--> 140\u001b[0;31m         biases, weights, verbose, validation_x, validation_y)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m def SGD(X_train, Y_train, epochs, mini_batch_size, lr, C, sizes, num_layers,\n",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X_train, Y_train, epochs, mini_batch_size, lr, C, sizes, num_layers, biases, weights, verbose, validation_x, validation_y)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m#feed-forward (and back) all mini_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(minibatch, lr, C, sizes, num_layers, biases, weights)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m#backpropagation for each observation in mini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;31m#print(delta_nabla_b.shape, 'delta_nabla_b ', delta_nabla_b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'delta_nabla_w '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FYS-STK4155/Project2/logisRegresANA.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(x, y, C, sizes, num_layers, biases, weights)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mtestyy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testyy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestyy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mnabla_w_backprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtestyy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnabla_b_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w_backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (1600,) "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import logisRegresANA\n",
    "importlib.reload(logisRegresANA)\n",
    "\n",
    "in_layer = X_train.shape[1] #number of neurons in the input layer\n",
    "\n",
    "if (len(Y_train.shape)==1): \n",
    "    out_layer = 1   #number of neurons in the output layer\n",
    "else: out_layer = Y_train.shape[1]\n",
    "biasesnn, weightsnn= logisRegresANA.neuralnetwork([in_layer, 10, out_layer], X_train, Y_train,\n",
    "                                                  validation_x=X_test, validation_y=Y_test,\n",
    "                                                  verbose=True,\n",
    "                             epochs= 30, mini_batch_size = 10, lr= 0.5, C='ce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Part f) Critical evaluation of the various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T13:14:31.851812Z",
     "start_time": "2018-11-13T13:14:25.253844Z"
    },
    "ipub": {
     "ignore": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:main:started ipypublish v0.6.7 at Tue Nov 13 14:14:25 2018\n",
      "INFO:main:logging to: /mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled.nbpub.log\n",
      "INFO:main:running for ipynb(s) at: Untitled.ipynb\n",
      "INFO:main:with conversion: latex_ipypublish_all\n",
      "INFO:nbmerge:Reading notebook\n",
      "INFO:main:getting output format from exporter plugin\n",
      "INFO:nbexport:running nbconvert\n",
      "INFO:latex_doc_defaults:adding ipub defaults to notebook\n",
      "INFO:split_outputs:splitting outputs into separate cells\n",
      "INFO:latex_doc_links:resolving external file paths in ipub metadata to: Untitled.ipynb\n",
      "INFO:latex_doc_captions:extracting caption cells\n",
      "INFO:main:outputting converted file to: /mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled.tex\n",
      "INFO:main:dumping external files to: /mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled_files\n",
      "INFO:main:running pdf conversion\n",
      "INFO:pdfexport:running: latexmk -xelatex -bibtex -pdf --interaction=batchmode <outpath>\n",
      "INFO:pdfexport:latexmk: Latexmk: This is Latexmk, John Collins, 17 Jan. 2018, version: 4.55.\n",
      "INFO:pdfexport:latexmk: Rule 'bibtex Untitled': File changes, etc:\n",
      "INFO:pdfexport:latexmk: Changed files, or newly in use since previous run(s):\n",
      "INFO:pdfexport:latexmk: 'Untitled.aux'\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: Run number 1 of rule 'bibtex Untitled'\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: Running 'bibtex  \"Untitled\"'\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: Latexmk: applying rule 'bibtex Untitled'...\n",
      "INFO:pdfexport:latexmk: For rule 'bibtex Untitled', running '&run_bibtex(  )' ...\n",
      "INFO:pdfexport:latexmk: This is BibTeX, Version 0.99d (TeX Live 2017/Debian)\n",
      "INFO:pdfexport:latexmk: The top-level auxiliary file: Untitled.aux\n",
      "INFO:pdfexport:latexmk: The style file: unsrtnat.bst\n",
      "INFO:pdfexport:latexmk: I found no \\citation commands---while reading file Untitled.aux\n",
      "INFO:pdfexport:latexmk: Database file #1: Untitled_files/ana.bib.bib\n",
      "INFO:pdfexport:latexmk: (There was 1 error message)\n",
      "INFO:pdfexport:latexmk: Rule 'pdflatex': File changes, etc:\n",
      "INFO:pdfexport:latexmk: Changed files, or newly in use since previous run(s):\n",
      "INFO:pdfexport:latexmk: 'Untitled.aux'\n",
      "INFO:pdfexport:latexmk: 'Untitled.out'\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: Run number 1 of rule 'pdflatex'\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: Running 'pdflatex  --interaction=batchmode -recorder  \"/mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled.tex\"'\n",
      "INFO:pdfexport:latexmk: ------------\n",
      "INFO:pdfexport:latexmk: Latexmk: applying rule 'pdflatex'...\n",
      "INFO:pdfexport:latexmk: This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017/Debian) (preloaded format=pdflatex)\n",
      "INFO:pdfexport:latexmk: restricted \\write18 enabled.\n",
      "INFO:pdfexport:latexmk: entering extended mode\n",
      "INFO:pdfexport:latexmk: Latexmk: Found input bbl file 'Untitled.bbl'\n",
      "INFO:pdfexport:latexmk: Latexmk: Log file says output to 'Untitled.pdf'\n",
      "INFO:pdfexport:latexmk: Latexmk: List of undefined refs and citations:\n",
      "INFO:pdfexport:latexmk: Label `code:example_code' multiply defined\n",
      "INFO:pdfexport:latexmk: Label `code:example_code@cref' multiply defined\n",
      "INFO:pdfexport:latexmk: === TeX engine is 'pdfTeX'\n",
      "INFO:pdfexport:latexmk: Latexmk: Found bibliography file(s) [Untitled_files/ana.bib]\n",
      "INFO:pdfexport:latexmk: Latexmk: Summary of warnings:\n",
      "INFO:pdfexport:latexmk: Latex found 4 multiply defined reference(s)\n",
      "INFO:pdfexport:latexmk: Collected error summary (may duplicate other messages):\n",
      "INFO:pdfexport:latexmk: pdflatex: Command for 'pdflatex' gave return code 1\n",
      "INFO:pdfexport:latexmk: Refer to 'Untitled.log' for details\n",
      "INFO:pdfexport:latexmk: Latexmk: Use the -f option to force complete processing,\n",
      "INFO:pdfexport:latexmk: unless error was exceeding maximum runs of latex/pdflatex.\n",
      "INFO:pdfexport:latexmk: Latexmk: Errors, so I did not complete making targets\n",
      "ERROR:pdfexport:pdf conversion failed: Try running with pdf_debug=True\n",
      "ERROR:main:pdf export returned false, try running with pdf_debug=True\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/nbpublish\", line 147, in <module>\n",
      "    nbpublish(filepath, **options)\n",
      "  File \"/opt/conda/bin/nbpublish\", line 70, in nbpublish\n",
      "    create_pdf=create_pdf, pdf_in_temp=pdf_in_temp, pdf_debug=pdf_debug)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipypublish/main.py\", line 161, in publish\n",
      "    raise RuntimeError('the pdf export failed, try running with pdf_debug=True')\n",
      "RuntimeError: the pdf export failed, try running with pdf_debug=True\n",
      "INFO:main:started ipypublish v0.6.7 at Tue Nov 13 14:14:30 2018\n",
      "INFO:main:logging to: /mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled.nbpub.log\n",
      "INFO:main:running for ipynb(s) at: Untitled.ipynb\n",
      "INFO:main:with conversion: latex_ipypublish_all\n",
      "INFO:nbmerge:Reading notebook\n",
      "INFO:main:getting output format from exporter plugin\n",
      "INFO:nbexport:running nbconvert\n",
      "INFO:latex_doc_defaults:adding ipub defaults to notebook\n",
      "INFO:split_outputs:splitting outputs into separate cells\n",
      "INFO:latex_doc_links:resolving external file paths in ipub metadata to: Untitled.ipynb\n",
      "INFO:latex_doc_captions:extracting caption cells\n",
      "INFO:main:outputting converted file to: /mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled.tex\n",
      "INFO:main:dumping external files to: /mnt/data/teachers/annefou/acos/project2-FYS-STK4155/converted/Untitled_files\n",
      "INFO:main:running pdf conversion\n",
      "INFO:pdfexport:running: latexmk -xelatex -bibtex -pdf --interaction=batchmode <outpath>\n",
      "INFO:pdfexport:latexmk: Latexmk: This is Latexmk, John Collins, 17 Jan. 2018, version: 4.55.\n",
      "INFO:pdfexport:latexmk: Latexmk: All targets (Untitled.pdf) are up-to-date\n",
      "INFO:pdfexport:pdf conversion complete\n",
      "INFO:main:process finished successfully\n"
     ]
    }
   ],
   "source": [
    "#This cell needs to be executed twice (for references)\n",
    "!nbpublish -f latex_ipypublish_all -pdf Untitled.ipynb\n",
    "!nbpublish -f latex_ipypublish_all -pdf Untitled.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "ipub": {
   "bibliography": "/mnt/data/teachers/annefou/acos/project2-FYS-STK4155/ana.bib",
   "titlepage": {
    "author": "Ana Costa",
    "email": "authors@email.com",
    "institution": [
     "UiO"
    ],
    "logo": "/mnt/data/teachers/annefou/acos/project2-FYS-STK4155/UiO_Segl_A.png",
    "subtitle": "Logistic Regression and Neural Network",
    "supervisors": [
     ""
    ],
    "tagline": "",
    "title": "Logistic Regression and Neural Network"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
